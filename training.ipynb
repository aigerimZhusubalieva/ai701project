{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turkey RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline Dataset Results:\n",
      "\n",
      "Fold 1 Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   alzheimer       0.33      0.33      0.33         9\n",
      "     control       0.57      0.73      0.64        11\n",
      "         mci       1.00      0.40      0.57         5\n",
      "\n",
      "    accuracy                           0.52        25\n",
      "   macro avg       0.63      0.49      0.51        25\n",
      "weighted avg       0.57      0.52      0.52        25\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3 6 0]\n",
      " [3 8 0]\n",
      " [3 0 2]]\n",
      "Fold 1 Accuracy: 0.52\n",
      "\n",
      "Fold 2 Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   alzheimer       0.29      0.20      0.24        10\n",
      "     control       0.29      0.40      0.33        10\n",
      "         mci       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.24        25\n",
      "   macro avg       0.19      0.20      0.19        25\n",
      "weighted avg       0.23      0.24      0.23        25\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2 6 2]\n",
      " [4 4 2]\n",
      " [1 4 0]]\n",
      "Fold 2 Accuracy: 0.24\n",
      "\n",
      "Fold 3 Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   alzheimer       0.29      0.20      0.24        10\n",
      "     control       0.43      0.60      0.50        10\n",
      "         mci       0.25      0.20      0.22         5\n",
      "\n",
      "    accuracy                           0.36        25\n",
      "   macro avg       0.32      0.33      0.32        25\n",
      "weighted avg       0.34      0.36      0.34        25\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2 6 2]\n",
      " [3 6 1]\n",
      " [2 2 1]]\n",
      "Fold 3 Accuracy: 0.36\n",
      "\n",
      "Fold 4 Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   alzheimer       0.43      0.33      0.38         9\n",
      "     control       0.50      0.60      0.55        10\n",
      "         mci       0.50      0.50      0.50         6\n",
      "\n",
      "    accuracy                           0.48        25\n",
      "   macro avg       0.48      0.48      0.47        25\n",
      "weighted avg       0.47      0.48      0.47        25\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3 5 1]\n",
      " [2 6 2]\n",
      " [2 1 3]]\n",
      "Fold 4 Accuracy: 0.48\n",
      "\n",
      "Fold 5 Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   alzheimer       0.45      0.56      0.50         9\n",
      "     control       0.57      0.40      0.47        10\n",
      "         mci       0.43      0.50      0.46         6\n",
      "\n",
      "    accuracy                           0.48        25\n",
      "   macro avg       0.48      0.49      0.48        25\n",
      "weighted avg       0.50      0.48      0.48        25\n",
      "\n",
      "Confusion Matrix:\n",
      "[[5 2 2]\n",
      " [4 4 2]\n",
      " [2 1 3]]\n",
      "Fold 5 Accuracy: 0.48\n",
      "\n",
      "K-Fold Cross-Validation Results:\n",
      "Mean Accuracy: 0.42\n",
      "Standard Deviation of Accuracy: 0.10\n",
      "\n",
      "Best Wrapper Dataset Results:\n",
      "\n",
      "Fold 1 Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   alzheimer       0.80      0.89      0.84         9\n",
      "     control       0.69      0.82      0.75        11\n",
      "         mci       1.00      0.40      0.57         5\n",
      "\n",
      "    accuracy                           0.76        25\n",
      "   macro avg       0.83      0.70      0.72        25\n",
      "weighted avg       0.79      0.76      0.75        25\n",
      "\n",
      "Confusion Matrix:\n",
      "[[8 1 0]\n",
      " [2 9 0]\n",
      " [0 3 2]]\n",
      "Fold 1 Accuracy: 0.76\n",
      "\n",
      "Fold 2 Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   alzheimer       0.71      0.50      0.59        10\n",
      "     control       0.67      0.80      0.73        10\n",
      "         mci       0.50      0.60      0.55         5\n",
      "\n",
      "    accuracy                           0.64        25\n",
      "   macro avg       0.63      0.63      0.62        25\n",
      "weighted avg       0.65      0.64      0.64        25\n",
      "\n",
      "Confusion Matrix:\n",
      "[[5 2 3]\n",
      " [2 8 0]\n",
      " [0 2 3]]\n",
      "Fold 2 Accuracy: 0.64\n",
      "\n",
      "Fold 3 Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   alzheimer       0.75      0.60      0.67        10\n",
      "     control       0.62      0.80      0.70        10\n",
      "         mci       0.50      0.40      0.44         5\n",
      "\n",
      "    accuracy                           0.64        25\n",
      "   macro avg       0.62      0.60      0.60        25\n",
      "weighted avg       0.65      0.64      0.63        25\n",
      "\n",
      "Confusion Matrix:\n",
      "[[6 3 1]\n",
      " [1 8 1]\n",
      " [1 2 2]]\n",
      "Fold 3 Accuracy: 0.64\n",
      "\n",
      "Fold 4 Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   alzheimer       0.50      0.33      0.40         9\n",
      "     control       0.53      0.80      0.64        10\n",
      "         mci       0.75      0.50      0.60         6\n",
      "\n",
      "    accuracy                           0.56        25\n",
      "   macro avg       0.59      0.54      0.55        25\n",
      "weighted avg       0.57      0.56      0.54        25\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3 5 1]\n",
      " [2 8 0]\n",
      " [1 2 3]]\n",
      "Fold 4 Accuracy: 0.56\n",
      "\n",
      "Fold 5 Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   alzheimer       0.57      0.44      0.50         9\n",
      "     control       0.62      0.80      0.70        10\n",
      "         mci       1.00      0.83      0.91         6\n",
      "\n",
      "    accuracy                           0.68        25\n",
      "   macro avg       0.73      0.69      0.70        25\n",
      "weighted avg       0.69      0.68      0.68        25\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4 5 0]\n",
      " [2 8 0]\n",
      " [1 0 5]]\n",
      "Fold 5 Accuracy: 0.68\n",
      "\n",
      "K-Fold Cross-Validation Results:\n",
      "Mean Accuracy: 0.66\n",
      "Standard Deviation of Accuracy: 0.06\n",
      "\n",
      "Correlation-Based Dataset Results:\n",
      "\n",
      "Fold 1 Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   alzheimer       0.62      0.56      0.59         9\n",
      "     control       0.64      0.82      0.72        11\n",
      "         mci       0.67      0.40      0.50         5\n",
      "\n",
      "    accuracy                           0.64        25\n",
      "   macro avg       0.64      0.59      0.60        25\n",
      "weighted avg       0.64      0.64      0.63        25\n",
      "\n",
      "Confusion Matrix:\n",
      "[[5 3 1]\n",
      " [2 9 0]\n",
      " [1 2 2]]\n",
      "Fold 1 Accuracy: 0.64\n",
      "\n",
      "Fold 2 Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   alzheimer       0.80      0.80      0.80        10\n",
      "     control       0.78      0.70      0.74        10\n",
      "         mci       0.83      1.00      0.91         5\n",
      "\n",
      "    accuracy                           0.80        25\n",
      "   macro avg       0.80      0.83      0.82        25\n",
      "weighted avg       0.80      0.80      0.80        25\n",
      "\n",
      "Confusion Matrix:\n",
      "[[8 2 0]\n",
      " [2 7 1]\n",
      " [0 0 5]]\n",
      "Fold 2 Accuracy: 0.80\n",
      "\n",
      "Fold 3 Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   alzheimer       0.64      0.70      0.67        10\n",
      "     control       0.70      0.70      0.70        10\n",
      "         mci       1.00      0.80      0.89         5\n",
      "\n",
      "    accuracy                           0.72        25\n",
      "   macro avg       0.78      0.73      0.75        25\n",
      "weighted avg       0.73      0.72      0.72        25\n",
      "\n",
      "Confusion Matrix:\n",
      "[[7 3 0]\n",
      " [3 7 0]\n",
      " [1 0 4]]\n",
      "Fold 3 Accuracy: 0.72\n",
      "\n",
      "Fold 4 Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   alzheimer       0.75      0.67      0.71         9\n",
      "     control       0.70      0.70      0.70        10\n",
      "         mci       0.71      0.83      0.77         6\n",
      "\n",
      "    accuracy                           0.72        25\n",
      "   macro avg       0.72      0.73      0.73        25\n",
      "weighted avg       0.72      0.72      0.72        25\n",
      "\n",
      "Confusion Matrix:\n",
      "[[6 3 0]\n",
      " [1 7 2]\n",
      " [1 0 5]]\n",
      "Fold 4 Accuracy: 0.72\n",
      "\n",
      "Fold 5 Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   alzheimer       0.67      0.89      0.76         9\n",
      "     control       0.86      0.60      0.71        10\n",
      "         mci       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           0.80        25\n",
      "   macro avg       0.84      0.83      0.82        25\n",
      "weighted avg       0.82      0.80      0.80        25\n",
      "\n",
      "Confusion Matrix:\n",
      "[[8 1 0]\n",
      " [4 6 0]\n",
      " [0 0 6]]\n",
      "Fold 5 Accuracy: 0.80\n",
      "\n",
      "K-Fold Cross-Validation Results:\n",
      "Mean Accuracy: 0.74\n",
      "Standard Deviation of Accuracy: 0.06\n"
     ]
    }
   ],
   "source": [
    "# Function to load and preprocess data\n",
    "def load_and_preprocess(data_path, features_path=None):\n",
    "    data = pd.read_csv(data_path)\n",
    "    if features_path:\n",
    "        selected_features = pd.read_csv(features_path)[\"Name\"].astype(str).tolist()\n",
    "        X = data[selected_features].apply(pd.to_numeric, errors='coerce').dropna(axis=1)\n",
    "    else:\n",
    "        X = data.drop(columns=['class']).apply(pd.to_numeric, errors='coerce')\n",
    "    y = data[\"class\"]\n",
    "    return X, y\n",
    "\n",
    "# Function to perform k-fold cross-validation\n",
    "def k_fold_train_evaluate_rf(X, y, n_splits=5):\n",
    "    # Encode target labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Hyperparameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    }\n",
    "\n",
    "    # Stratified K-Fold cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    fold_accuracies = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(X_scaled, y_encoded), 1):\n",
    "        # Split into training and test sets for this fold\n",
    "        X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "        y_train, y_test = y_encoded[train_idx], y_encoded[test_idx]\n",
    "\n",
    "        # Perform GridSearchCV for hyperparameter tuning\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=RandomForestClassifier(random_state=42),\n",
    "            param_grid=param_grid,\n",
    "            scoring='accuracy',\n",
    "            cv=5,\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Best model from GridSearchCV\n",
    "        best_rf_model = grid_search.best_estimator_\n",
    "        y_pred = best_rf_model.predict(X_test)\n",
    "\n",
    "        # Calculate accuracy for this fold\n",
    "        fold_accuracy = accuracy_score(y_test, y_pred)\n",
    "        fold_accuracies.append(fold_accuracy)\n",
    "\n",
    "        # Print results for this fold\n",
    "        print(f\"\\nFold {fold} Results:\")\n",
    "        print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "        print(f\"Fold {fold} Accuracy: {fold_accuracy:.2f}\")\n",
    "\n",
    "    # Overall results\n",
    "    print(\"\\nK-Fold Cross-Validation Results:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(fold_accuracies):.2f}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(fold_accuracies):.2f}\")\n",
    "\n",
    "# Baseline Dataset\n",
    "print(\"\\nBaseline Dataset Results:\")\n",
    "X, y = load_and_preprocess(\"baseline_turkey.csv\")\n",
    "k_fold_train_evaluate_rf(X, y)\n",
    "\n",
    "# Best Wrapper Dataset\n",
    "print(\"\\nBest Wrapper Dataset Results:\")\n",
    "X, y = load_and_preprocess(\"baseline_turkey.csv\", \"best_first_wrapper_turkey.csv\")\n",
    "k_fold_train_evaluate_rf(X, y)\n",
    "\n",
    "# Correlation-Based Dataset\n",
    "print(\"\\nCorrelation-Based Dataset Results:\")\n",
    "X, y = load_and_preprocess(\"baseline_turkey.csv\", \"best_first_cor_turkey.csv\")\n",
    "k_fold_train_evaluate_rf(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hangzhou RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline Dataset Results:\n",
      "\n",
      "Fold 1 Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   alzheimer       0.36      0.67      0.47         6\n",
      "     control       0.00      0.00      0.00         6\n",
      "         mci       0.43      0.43      0.43         7\n",
      "\n",
      "    accuracy                           0.37        19\n",
      "   macro avg       0.26      0.37      0.30        19\n",
      "weighted avg       0.27      0.37      0.31        19\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4 0 2]\n",
      " [4 0 2]\n",
      " [3 1 3]]\n",
      "Fold 1 Accuracy: 0.37\n",
      "\n",
      "Fold 2 Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   alzheimer       0.36      0.83      0.50         6\n",
      "     control       1.00      0.17      0.29         6\n",
      "         mci       0.75      0.43      0.55         7\n",
      "\n",
      "    accuracy                           0.47        19\n",
      "   macro avg       0.70      0.48      0.44        19\n",
      "weighted avg       0.70      0.47      0.45        19\n",
      "\n",
      "Confusion Matrix:\n",
      "[[5 0 1]\n",
      " [5 1 0]\n",
      " [4 0 3]]\n",
      "Fold 2 Accuracy: 0.47\n",
      "\n",
      "Fold 3 Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   alzheimer       0.50      0.43      0.46         7\n",
      "     control       0.50      0.20      0.29         5\n",
      "         mci       0.50      0.83      0.62         6\n",
      "\n",
      "    accuracy                           0.50        18\n",
      "   macro avg       0.50      0.49      0.46        18\n",
      "weighted avg       0.50      0.50      0.47        18\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3 1 3]\n",
      " [2 1 2]\n",
      " [1 0 5]]\n",
      "Fold 3 Accuracy: 0.50\n",
      "\n",
      "Fold 4 Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   alzheimer       0.33      0.43      0.38         7\n",
      "     control       0.00      0.00      0.00         5\n",
      "         mci       0.22      0.33      0.27         6\n",
      "\n",
      "    accuracy                           0.28        18\n",
      "   macro avg       0.19      0.25      0.21        18\n",
      "weighted avg       0.20      0.28      0.23        18\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3 0 4]\n",
      " [2 0 3]\n",
      " [4 0 2]]\n",
      "Fold 4 Accuracy: 0.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/qiime2-amplicon-2024.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/qiime2-amplicon-2024.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/qiime2-amplicon-2024.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 5 Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   alzheimer       0.50      0.86      0.63         7\n",
      "     control       0.50      0.40      0.44         5\n",
      "         mci       0.50      0.17      0.25         6\n",
      "\n",
      "    accuracy                           0.50        18\n",
      "   macro avg       0.50      0.47      0.44        18\n",
      "weighted avg       0.50      0.50      0.45        18\n",
      "\n",
      "Confusion Matrix:\n",
      "[[6 1 0]\n",
      " [2 2 1]\n",
      " [4 1 1]]\n",
      "Fold 5 Accuracy: 0.50\n",
      "\n",
      "K-Fold Cross-Validation Results:\n",
      "Mean Accuracy: 0.42\n",
      "Standard Deviation of Accuracy: 0.09\n",
      "\n",
      "Best Wrapper Dataset Results:\n",
      "\n",
      "Fold 1 Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   alzheimer       0.62      0.83      0.71         6\n",
      "     control       0.60      0.50      0.55         6\n",
      "         mci       0.67      0.57      0.62         7\n",
      "\n",
      "    accuracy                           0.63        19\n",
      "   macro avg       0.63      0.63      0.63        19\n",
      "weighted avg       0.63      0.63      0.62        19\n",
      "\n",
      "Confusion Matrix:\n",
      "[[5 1 0]\n",
      " [1 3 2]\n",
      " [2 1 4]]\n",
      "Fold 1 Accuracy: 0.63\n",
      "\n",
      "Fold 2 Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   alzheimer       1.00      0.83      0.91         6\n",
      "     control       0.80      0.67      0.73         6\n",
      "         mci       0.78      1.00      0.88         7\n",
      "\n",
      "    accuracy                           0.84        19\n",
      "   macro avg       0.86      0.83      0.84        19\n",
      "weighted avg       0.85      0.84      0.84        19\n",
      "\n",
      "Confusion Matrix:\n",
      "[[5 1 0]\n",
      " [0 4 2]\n",
      " [0 0 7]]\n",
      "Fold 2 Accuracy: 0.84\n",
      "\n",
      "Fold 3 Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   alzheimer       0.62      0.71      0.67         7\n",
      "     control       0.60      0.60      0.60         5\n",
      "         mci       0.60      0.50      0.55         6\n",
      "\n",
      "    accuracy                           0.61        18\n",
      "   macro avg       0.61      0.60      0.60        18\n",
      "weighted avg       0.61      0.61      0.61        18\n",
      "\n",
      "Confusion Matrix:\n",
      "[[5 1 1]\n",
      " [1 3 1]\n",
      " [2 1 3]]\n",
      "Fold 3 Accuracy: 0.61\n",
      "\n",
      "Fold 4 Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   alzheimer       0.67      0.57      0.62         7\n",
      "     control       0.50      0.20      0.29         5\n",
      "         mci       0.50      0.83      0.62         6\n",
      "\n",
      "    accuracy                           0.56        18\n",
      "   macro avg       0.56      0.53      0.51        18\n",
      "weighted avg       0.56      0.56      0.53        18\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4 1 2]\n",
      " [1 1 3]\n",
      " [1 0 5]]\n",
      "Fold 4 Accuracy: 0.56\n",
      "\n",
      "Fold 5 Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   alzheimer       0.83      0.71      0.77         7\n",
      "     control       0.67      0.40      0.50         5\n",
      "         mci       0.56      0.83      0.67         6\n",
      "\n",
      "    accuracy                           0.67        18\n",
      "   macro avg       0.69      0.65      0.65        18\n",
      "weighted avg       0.69      0.67      0.66        18\n",
      "\n",
      "Confusion Matrix:\n",
      "[[5 1 1]\n",
      " [0 2 3]\n",
      " [1 0 5]]\n",
      "Fold 5 Accuracy: 0.67\n",
      "\n",
      "K-Fold Cross-Validation Results:\n",
      "Mean Accuracy: 0.66\n",
      "Standard Deviation of Accuracy: 0.10\n",
      "\n",
      "Correlation-Based Dataset Results:\n",
      "\n",
      "Fold 1 Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   alzheimer       0.71      0.83      0.77         6\n",
      "     control       1.00      1.00      1.00         6\n",
      "         mci       0.83      0.71      0.77         7\n",
      "\n",
      "    accuracy                           0.84        19\n",
      "   macro avg       0.85      0.85      0.85        19\n",
      "weighted avg       0.85      0.84      0.84        19\n",
      "\n",
      "Confusion Matrix:\n",
      "[[5 0 1]\n",
      " [0 6 0]\n",
      " [2 0 5]]\n",
      "Fold 1 Accuracy: 0.84\n",
      "\n",
      "Fold 2 Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   alzheimer       0.71      0.83      0.77         6\n",
      "     control       0.75      0.50      0.60         6\n",
      "         mci       0.88      1.00      0.93         7\n",
      "\n",
      "    accuracy                           0.79        19\n",
      "   macro avg       0.78      0.78      0.77        19\n",
      "weighted avg       0.78      0.79      0.78        19\n",
      "\n",
      "Confusion Matrix:\n",
      "[[5 1 0]\n",
      " [2 3 1]\n",
      " [0 0 7]]\n",
      "Fold 2 Accuracy: 0.79\n",
      "\n",
      "Fold 3 Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   alzheimer       0.56      0.71      0.62         7\n",
      "     control       0.60      0.60      0.60         5\n",
      "         mci       0.75      0.50      0.60         6\n",
      "\n",
      "    accuracy                           0.61        18\n",
      "   macro avg       0.64      0.60      0.61        18\n",
      "weighted avg       0.63      0.61      0.61        18\n",
      "\n",
      "Confusion Matrix:\n",
      "[[5 1 1]\n",
      " [2 3 0]\n",
      " [2 1 3]]\n",
      "Fold 3 Accuracy: 0.61\n",
      "\n",
      "Fold 4 Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   alzheimer       0.50      0.43      0.46         7\n",
      "     control       0.50      0.40      0.44         5\n",
      "         mci       0.50      0.67      0.57         6\n",
      "\n",
      "    accuracy                           0.50        18\n",
      "   macro avg       0.50      0.50      0.49        18\n",
      "weighted avg       0.50      0.50      0.49        18\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3 2 2]\n",
      " [1 2 2]\n",
      " [2 0 4]]\n",
      "Fold 4 Accuracy: 0.50\n",
      "\n",
      "Fold 5 Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   alzheimer       0.78      1.00      0.88         7\n",
      "     control       1.00      0.40      0.57         5\n",
      "         mci       0.57      0.67      0.62         6\n",
      "\n",
      "    accuracy                           0.72        18\n",
      "   macro avg       0.78      0.69      0.69        18\n",
      "weighted avg       0.77      0.72      0.70        18\n",
      "\n",
      "Confusion Matrix:\n",
      "[[7 0 0]\n",
      " [0 2 3]\n",
      " [2 0 4]]\n",
      "Fold 5 Accuracy: 0.72\n",
      "\n",
      "K-Fold Cross-Validation Results:\n",
      "Mean Accuracy: 0.69\n",
      "Standard Deviation of Accuracy: 0.12\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_and_preprocess(data_path, features_path=None):\n",
    "    data = pd.read_csv(data_path)\n",
    "    if features_path:\n",
    "        selected_features = pd.read_csv(features_path)[\"Name\"].astype(str).tolist()\n",
    "        X = data[selected_features].apply(pd.to_numeric, errors='coerce').dropna(axis=1)\n",
    "    else:\n",
    "        X = data.drop(columns=['class']).apply(pd.to_numeric, errors='coerce')\n",
    "    y = data[\"class\"]\n",
    "    return X, y\n",
    "\n",
    "# Function to perform k-fold cross-validation\n",
    "def k_fold_train_evaluate_rf(X, y, n_splits=5):\n",
    "    # Encode target labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Hyperparameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    }\n",
    "\n",
    "    # Stratified K-Fold cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    fold_accuracies = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(X_scaled, y_encoded), 1):\n",
    "        # Split into training and test sets for this fold\n",
    "        X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "        y_train, y_test = y_encoded[train_idx], y_encoded[test_idx]\n",
    "\n",
    "        # Perform GridSearchCV for hyperparameter tuning\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=RandomForestClassifier(random_state=42),\n",
    "            param_grid=param_grid,\n",
    "            scoring='accuracy',\n",
    "            cv=5,\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Best model from GridSearchCV\n",
    "        best_rf_model = grid_search.best_estimator_\n",
    "        y_pred = best_rf_model.predict(X_test)\n",
    "\n",
    "        # Calculate accuracy for this fold\n",
    "        fold_accuracy = accuracy_score(y_test, y_pred)\n",
    "        fold_accuracies.append(fold_accuracy)\n",
    "\n",
    "        # Print results for this fold\n",
    "        print(f\"\\nFold {fold} Results:\")\n",
    "        print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "        print(f\"Fold {fold} Accuracy: {fold_accuracy:.2f}\")\n",
    "\n",
    "    # Overall results\n",
    "    print(\"\\nK-Fold Cross-Validation Results:\")\n",
    "    print(f\"Mean Accuracy: {np.mean(fold_accuracies):.2f}\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(fold_accuracies):.2f}\")\n",
    "\n",
    "# Baseline Dataset\n",
    "print(\"\\nBaseline Dataset Results:\")\n",
    "X, y = load_and_preprocess(\"baseline_china.csv\")\n",
    "k_fold_train_evaluate_rf(X, y)\n",
    "\n",
    "# Best Wrapper Dataset\n",
    "print(\"\\nBest Wrapper Dataset Results:\")\n",
    "X, y = load_and_preprocess(\"baseline_china.csv\", \"updated_best_first_wrapper_china.csv\")\n",
    "k_fold_train_evaluate_rf(X, y)\n",
    "\n",
    "# Correlation-Based Dataset\n",
    "print(\"\\nCorrelation-Based Dataset Results:\")\n",
    "X, y = load_and_preprocess(\"baseline_china.csv\", \"updated_best_first_cor_china.csv\")\n",
    "k_fold_train_evaluate_rf(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turkey - SVM, KNN, NB, LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models on Baseline Dataset with 5-fold cross-validation...\n",
      "\n",
      "Model: SVM\n",
      "Mean Accuracy (5-fold CV): 0.45\n",
      "Model: KNN\n",
      "Mean Accuracy (5-fold CV): 0.47\n",
      "Model: Naive Bayes\n",
      "Mean Accuracy (5-fold CV): 0.49\n",
      "Model: Logistic Regression\n",
      "Mean Accuracy (5-fold CV): 0.41\n",
      "\n",
      "Training models on Correlation-Based Dataset with 5-fold cross-validation...\n",
      "\n",
      "Model: SVM\n",
      "Mean Accuracy (5-fold CV): 0.54\n",
      "Model: KNN\n",
      "Mean Accuracy (5-fold CV): 0.50\n",
      "Model: Naive Bayes\n",
      "Mean Accuracy (5-fold CV): 0.78\n",
      "Model: Logistic Regression\n",
      "Mean Accuracy (5-fold CV): 0.63\n",
      "\n",
      "Training models on Wrapper Dataset with 5-fold cross-validation...\n",
      "\n",
      "Model: SVM\n",
      "Mean Accuracy (5-fold CV): 0.45\n",
      "Model: KNN\n",
      "Mean Accuracy (5-fold CV): 0.46\n",
      "Model: Naive Bayes\n",
      "Mean Accuracy (5-fold CV): 0.56\n",
      "Model: Logistic Regression\n",
      "Mean Accuracy (5-fold CV): 0.48\n",
      "\n",
      "Summary of Model Performance for Each Dataset (5-fold CV):\n",
      "\n",
      "Baseline Dataset:\n",
      "SVM: 0.45\n",
      "KNN: 0.47\n",
      "Naive Bayes: 0.49\n",
      "Logistic Regression: 0.41\n",
      "\n",
      "Correlation-Based Dataset:\n",
      "SVM: 0.54\n",
      "KNN: 0.50\n",
      "Naive Bayes: 0.78\n",
      "Logistic Regression: 0.63\n",
      "\n",
      "Wrapper Dataset:\n",
      "SVM: 0.45\n",
      "KNN: 0.46\n",
      "Naive Bayes: 0.56\n",
      "Logistic Regression: 0.48\n"
     ]
    }
   ],
   "source": [
    "# load the datasets\n",
    "baseline_data = pd.read_csv(\"baseline_turkey.csv\")\n",
    "cor_based_data = pd.read_csv(\"best_first_cor_turkey.csv\")\n",
    "cfs_data = pd.read_csv(\"best_first_wrapper_turkey.csv\")\n",
    "\n",
    "# prepare the baseline dataset\n",
    "X_baseline = baseline_data.drop(columns=['class']) \n",
    "y_baseline = baseline_data['class']  \n",
    "\n",
    "# prepare the correlation-based dataset\n",
    "cor_features = cor_based_data['Name'].astype(str).tolist()  \n",
    "cor_features = [col for col in cor_features if col in X_baseline.columns]  \n",
    "X_cor = baseline_data[cor_features]  \n",
    "y_cor = y_baseline  \n",
    "\n",
    "# prepare the wrapper dataset\n",
    "cfs_features = cfs_data['Name'].astype(str).tolist()  \n",
    "cfs_features = [col for col in cfs_features if col in X_baseline.columns]  \n",
    "X_cfs = baseline_data[cfs_features] \n",
    "y_cfs = y_baseline  \n",
    "\n",
    "# convert all features to numeric\n",
    "X_baseline = X_baseline.apply(pd.to_numeric, errors='coerce')  \n",
    "X_cor = X_cor.apply(pd.to_numeric, errors='coerce')  \n",
    "X_cfs = X_cfs.apply(pd.to_numeric, errors='coerce')  \n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"mean\")  # Replace NaNs with column mean\n",
    "X_baseline = pd.DataFrame(imputer.fit_transform(X_baseline), columns=X_baseline.columns)\n",
    "X_cor = pd.DataFrame(imputer.fit_transform(X_cor), columns=X_cor.columns)\n",
    "X_cfs = pd.DataFrame(imputer.fit_transform(X_cfs), columns=X_cfs.columns)\n",
    "\n",
    "# align target labels\n",
    "y_baseline = y_baseline.loc[X_baseline.index]\n",
    "y_cor = y_cor.loc[X_cor.index]\n",
    "y_cfs = y_cfs.loc[X_cfs.index]\n",
    "\n",
    "# encode target labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "datasets = {\n",
    "    \"Baseline Dataset\": (X_baseline, label_encoder.fit_transform(y_baseline)),\n",
    "    \"Correlation-Based Dataset\": (X_cor, label_encoder.fit_transform(y_cor)),\n",
    "    \"Wrapper Dataset\": (X_cfs, label_encoder.fit_transform(y_cfs)),\n",
    "}\n",
    "\n",
    "\n",
    "models = {\n",
    "    \"SVM\": SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000)\n",
    "}\n",
    "\n",
    "# train and evaluate each model using 5-fold CV\n",
    "results = {}\n",
    "\n",
    "for dataset_name, (X, y) in datasets.items():\n",
    "    print(f\"\\nTraining models on {dataset_name} with 5-fold cross-validation...\\n\")\n",
    "    \n",
    "    if len(X) == 0 or len(y) == 0:\n",
    "        print(f\"Skipping {dataset_name}: No valid samples after preprocessing.\\n\")\n",
    "        continue\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        try:\n",
    "            accuracies = []\n",
    "            \n",
    "            # perform 5-fold cross-validation\n",
    "            for train_index, test_index in skf.split(X, y):\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "                \n",
    "                model.fit(X_train, y_train)\n",
    "                \n",
    "                y_pred = model.predict(X_test)\n",
    "                \n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                accuracies.append(accuracy)\n",
    "            \n",
    "            mean_accuracy = sum(accuracies) / len(accuracies)\n",
    "            \n",
    "            if dataset_name not in results:\n",
    "                results[dataset_name] = {}\n",
    "            results[dataset_name][model_name] = mean_accuracy\n",
    "            \n",
    "            print(f\"Model: {model_name}\")\n",
    "            print(f\"Mean Accuracy (5-fold CV): {mean_accuracy:.2f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error training {model_name} on {dataset_name}: {e}\")\n",
    "\n",
    "# summary of results\n",
    "print(\"\\nSummary of Model Performance for Each Dataset (5-fold CV):\")\n",
    "for dataset_name, model_accuracies in results.items():\n",
    "    print(f\"\\n{dataset_name}:\")\n",
    "    for model_name, accuracy in model_accuracies.items():\n",
    "        print(f\"{model_name}: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models on Baseline Dataset...\n",
      "\n",
      "Model: SVM\n",
      "Accuracy: 0.37\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.43      0.40        14\n",
      "           1       0.38      0.50      0.43        16\n",
      "           2       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.37        38\n",
      "   macro avg       0.25      0.31      0.28        38\n",
      "weighted avg       0.30      0.37      0.33        38\n",
      "\n",
      "Model: KNN\n",
      "Accuracy: 0.34\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.43      0.38        14\n",
      "           1       0.40      0.25      0.31        16\n",
      "           2       0.30      0.38      0.33         8\n",
      "\n",
      "    accuracy                           0.34        38\n",
      "   macro avg       0.34      0.35      0.34        38\n",
      "weighted avg       0.35      0.34      0.34        38\n",
      "\n",
      "Model: Naive Bayes\n",
      "Accuracy: 0.53\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.57      0.52        14\n",
      "           1       0.50      0.50      0.50        16\n",
      "           2       0.80      0.50      0.62         8\n",
      "\n",
      "    accuracy                           0.53        38\n",
      "   macro avg       0.59      0.52      0.54        38\n",
      "weighted avg       0.55      0.53      0.53        38\n",
      "\n",
      "Model: Logistic Regression\n",
      "Accuracy: 0.34\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.21      0.23        14\n",
      "           1       0.41      0.44      0.42        16\n",
      "           2       0.33      0.38      0.35         8\n",
      "\n",
      "    accuracy                           0.34        38\n",
      "   macro avg       0.33      0.34      0.34        38\n",
      "weighted avg       0.34      0.34      0.34        38\n",
      "\n",
      "\n",
      "Training models on Correlation-Based Dataset...\n",
      "\n",
      "Model: SVM\n",
      "Accuracy: 0.53\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.29      0.40        14\n",
      "           1       0.52      0.94      0.67        16\n",
      "           2       0.33      0.12      0.18         8\n",
      "\n",
      "    accuracy                           0.53        38\n",
      "   macro avg       0.51      0.45      0.42        38\n",
      "weighted avg       0.53      0.53      0.47        38\n",
      "\n",
      "Model: KNN\n",
      "Accuracy: 0.47\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.36      0.34        14\n",
      "           1       0.50      0.50      0.50        16\n",
      "           2       0.71      0.62      0.67         8\n",
      "\n",
      "    accuracy                           0.47        38\n",
      "   macro avg       0.52      0.49      0.50        38\n",
      "weighted avg       0.48      0.47      0.48        38\n",
      "\n",
      "Model: Naive Bayes\n",
      "Accuracy: 0.66\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.50      0.61        14\n",
      "           1       0.77      0.62      0.69        16\n",
      "           2       0.50      1.00      0.67         8\n",
      "\n",
      "    accuracy                           0.66        38\n",
      "   macro avg       0.68      0.71      0.66        38\n",
      "weighted avg       0.72      0.66      0.65        38\n",
      "\n",
      "Model: Logistic Regression\n",
      "Accuracy: 0.74\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.57      0.64        14\n",
      "           1       0.74      0.88      0.80        16\n",
      "           2       0.75      0.75      0.75         8\n",
      "\n",
      "    accuracy                           0.74        38\n",
      "   macro avg       0.74      0.73      0.73        38\n",
      "weighted avg       0.74      0.74      0.73        38\n",
      "\n",
      "\n",
      "Training models on CFS Dataset...\n",
      "\n",
      "Model: SVM\n",
      "Accuracy: 0.53\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.07      0.12        14\n",
      "           1       0.50      0.88      0.64        16\n",
      "           2       0.71      0.62      0.67         8\n",
      "\n",
      "    accuracy                           0.53        38\n",
      "   macro avg       0.52      0.52      0.47        38\n",
      "weighted avg       0.48      0.53      0.45        38\n",
      "\n",
      "Model: KNN\n",
      "Accuracy: 0.53\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.36      0.40        14\n",
      "           1       0.50      0.69      0.58        16\n",
      "           2       0.80      0.50      0.62         8\n",
      "\n",
      "    accuracy                           0.53        38\n",
      "   macro avg       0.58      0.51      0.53        38\n",
      "weighted avg       0.55      0.53      0.52        38\n",
      "\n",
      "Model: Naive Bayes\n",
      "Accuracy: 0.50\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.07      0.12        14\n",
      "           1       0.50      0.81      0.62        16\n",
      "           2       0.56      0.62      0.59         8\n",
      "\n",
      "    accuracy                           0.50        38\n",
      "   macro avg       0.46      0.50      0.44        38\n",
      "weighted avg       0.45      0.50      0.43        38\n",
      "\n",
      "Model: Logistic Regression\n",
      "Accuracy: 0.50\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.14      0.19        14\n",
      "           1       0.50      0.81      0.62        16\n",
      "           2       0.80      0.50      0.62         8\n",
      "\n",
      "    accuracy                           0.50        38\n",
      "   macro avg       0.53      0.49      0.47        38\n",
      "weighted avg       0.48      0.50      0.46        38\n",
      "\n",
      "\n",
      "Summary of Model Performance for Each Dataset:\n",
      "\n",
      "Baseline Dataset:\n",
      "SVM: 0.37\n",
      "KNN: 0.34\n",
      "Naive Bayes: 0.53\n",
      "Logistic Regression: 0.34\n",
      "\n",
      "Correlation-Based Dataset:\n",
      "SVM: 0.53\n",
      "KNN: 0.47\n",
      "Naive Bayes: 0.66\n",
      "Logistic Regression: 0.74\n",
      "\n",
      "CFS Dataset:\n",
      "SVM: 0.53\n",
      "KNN: 0.53\n",
      "Naive Bayes: 0.50\n",
      "Logistic Regression: 0.50\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the datasets\n",
    "baseline_data = pd.read_csv(\"baseline_turkey.csv\")\n",
    "cor_based_data = pd.read_csv(\"best_first_cor_turkey.csv\")\n",
    "cfs_data = pd.read_csv(\"best_first_wrapper_turkey.csv\")\n",
    "\n",
    "# Step 2: Prepare the Baseline Dataset\n",
    "X_baseline = baseline_data.drop(columns=['class'])  # Features\n",
    "y_baseline = baseline_data['class']  # Target labels\n",
    "\n",
    "# Step 3: Prepare the Correlation-Based Dataset\n",
    "cor_features = cor_based_data['Name'].astype(str).tolist()  # Feature names\n",
    "cor_features = [col for col in cor_features if col in X_baseline.columns]  # Align features\n",
    "X_cor = baseline_data[cor_features]  # Features\n",
    "y_cor = y_baseline  # Target labels\n",
    "\n",
    "# Step 4: Prepare the CFS Dataset\n",
    "cfs_features = cfs_data['Name'].astype(str).tolist()  # Feature names\n",
    "cfs_features = [col for col in cfs_features if col in X_baseline.columns]  # Align features\n",
    "X_cfs = baseline_data[cfs_features]  # Features\n",
    "y_cfs = y_baseline  # Target labels\n",
    "\n",
    "# Step 5: Convert all features to numeric\n",
    "X_baseline = X_baseline.apply(pd.to_numeric, errors='coerce')  # Convert to numeric\n",
    "X_cor = X_cor.apply(pd.to_numeric, errors='coerce')  # Convert to numeric\n",
    "X_cfs = X_cfs.apply(pd.to_numeric, errors='coerce')  # Convert to numeric\n",
    "\n",
    "# Step 6: Handle missing values (impute instead of drop)\n",
    "imputer = SimpleImputer(strategy=\"mean\")  # Replace NaNs with column mean\n",
    "X_baseline = pd.DataFrame(imputer.fit_transform(X_baseline), columns=X_baseline.columns)\n",
    "X_cor = pd.DataFrame(imputer.fit_transform(X_cor), columns=X_cor.columns)\n",
    "X_cfs = pd.DataFrame(imputer.fit_transform(X_cfs), columns=X_cfs.columns)\n",
    "\n",
    "# Align target labels\n",
    "y_baseline = y_baseline.loc[X_baseline.index]\n",
    "y_cor = y_cor.loc[X_cor.index]\n",
    "y_cfs = y_cfs.loc[X_cfs.index]\n",
    "\n",
    "# Step 7: Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "datasets = {\n",
    "    \"Baseline Dataset\": (X_baseline, label_encoder.fit_transform(y_baseline)),\n",
    "    \"Correlation-Based Dataset\": (X_cor, label_encoder.fit_transform(y_cor)),\n",
    "    \"CFS Dataset\": (X_cfs, label_encoder.fit_transform(y_cfs)),\n",
    "}\n",
    "\n",
    "# Step 8: Initialize algorithms\n",
    "models = {\n",
    "    \"SVM\": SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000)\n",
    "}\n",
    "\n",
    "# Step 9: Train and evaluate each model on all datasets\n",
    "results = {}\n",
    "\n",
    "for dataset_name, (X, y) in datasets.items():\n",
    "    print(f\"\\nTraining models on {dataset_name}...\\n\")\n",
    "    \n",
    "    if len(X) == 0 or len(y) == 0:\n",
    "        print(f\"Skipping {dataset_name}: No valid samples after preprocessing.\\n\")\n",
    "        continue\n",
    "    \n",
    "    # Split the data into training and testing subsets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        try:\n",
    "            # Train the model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            # Evaluate the model\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            \n",
    "            # Store the results\n",
    "            if dataset_name not in results:\n",
    "                results[dataset_name] = {}\n",
    "            results[dataset_name][model_name] = accuracy\n",
    "            \n",
    "            # Generate target names dynamically\n",
    "            target_names = [str(cls) for cls in sorted(set(y_train))]\n",
    "            \n",
    "            print(f\"Model: {model_name}\")\n",
    "            print(f\"Accuracy: {accuracy:.2f}\")\n",
    "            print(\"Classification Report:\")\n",
    "            print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "        except Exception as e:\n",
    "            print(f\"Error training {model_name} on {dataset_name}: {e}\")\n",
    "\n",
    "# Step 10: Display summary of results\n",
    "print(\"\\nSummary of Model Performance for Each Dataset:\")\n",
    "for dataset_name, model_accuracies in results.items():\n",
    "    print(f\"\\n{dataset_name}:\")\n",
    "    for model_name, accuracy in model_accuracies.items():\n",
    "        print(f\"{model_name}: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### China - SVM, KNN, NB, LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models on Baseline Dataset with 5-fold cross-validation...\n",
      "\n",
      "Model: SVM\n",
      "Mean Accuracy (5-fold CV): 0.27\n",
      "Model: KNN\n",
      "Mean Accuracy (5-fold CV): 0.27\n",
      "Model: Naive Bayes\n",
      "Mean Accuracy (5-fold CV): 0.29\n",
      "Model: Logistic Regression\n",
      "Mean Accuracy (5-fold CV): 0.31\n",
      "\n",
      "Training models on Correlation-Based Dataset with 5-fold cross-validation...\n",
      "\n",
      "Model: SVM\n",
      "Mean Accuracy (5-fold CV): 0.60\n",
      "Model: KNN\n",
      "Mean Accuracy (5-fold CV): 0.51\n",
      "Model: Naive Bayes\n",
      "Mean Accuracy (5-fold CV): 0.61\n",
      "Model: Logistic Regression\n",
      "Mean Accuracy (5-fold CV): 0.57\n",
      "\n",
      "Training models on Wrapper Dataset with 5-fold cross-validation...\n",
      "\n",
      "Model: SVM\n",
      "Mean Accuracy (5-fold CV): 0.57\n",
      "Model: KNN\n",
      "Mean Accuracy (5-fold CV): 0.61\n",
      "Model: Naive Bayes\n",
      "Mean Accuracy (5-fold CV): 0.63\n",
      "Model: Logistic Regression\n",
      "Mean Accuracy (5-fold CV): 0.62\n",
      "\n",
      "Summary of Model Performance for Each Dataset (5-fold CV):\n",
      "\n",
      "Baseline Dataset:\n",
      "SVM: 0.27\n",
      "KNN: 0.27\n",
      "Naive Bayes: 0.29\n",
      "Logistic Regression: 0.31\n",
      "\n",
      "Correlation-Based Dataset:\n",
      "SVM: 0.60\n",
      "KNN: 0.51\n",
      "Naive Bayes: 0.61\n",
      "Logistic Regression: 0.57\n",
      "\n",
      "Wrapper Dataset:\n",
      "SVM: 0.57\n",
      "KNN: 0.61\n",
      "Naive Bayes: 0.63\n",
      "Logistic Regression: 0.62\n"
     ]
    }
   ],
   "source": [
    "# load the datasets\n",
    "baseline_data = pd.read_csv(\"baseline_china.csv\")\n",
    "cor_based_data = pd.read_csv(\"updated_best_first_cor_china.csv\")\n",
    "cfs_data = pd.read_csv(\"updated_best_first_wrapper_china.csv\")\n",
    "\n",
    "# prepare the baseline dataset\n",
    "X_baseline = baseline_data.drop(columns=['class']) \n",
    "y_baseline = baseline_data['class']  \n",
    "\n",
    "# prepare the correlation-based dataset\n",
    "cor_features = cor_based_data['Name'].astype(str).tolist()  \n",
    "cor_features = [col for col in cor_features if col in X_baseline.columns]  \n",
    "X_cor = baseline_data[cor_features]  \n",
    "y_cor = y_baseline  \n",
    "\n",
    "# prepare the wrapper dataset\n",
    "cfs_features = cfs_data['Name'].astype(str).tolist()  \n",
    "cfs_features = [col for col in cfs_features if col in X_baseline.columns]  \n",
    "X_cfs = baseline_data[cfs_features] \n",
    "y_cfs = y_baseline  \n",
    "\n",
    "# convert all features to numeric\n",
    "X_baseline = X_baseline.apply(pd.to_numeric, errors='coerce')  \n",
    "X_cor = X_cor.apply(pd.to_numeric, errors='coerce')  \n",
    "X_cfs = X_cfs.apply(pd.to_numeric, errors='coerce')  \n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"mean\")  # Replace NaNs with column mean\n",
    "X_baseline = pd.DataFrame(imputer.fit_transform(X_baseline), columns=X_baseline.columns)\n",
    "X_cor = pd.DataFrame(imputer.fit_transform(X_cor), columns=X_cor.columns)\n",
    "X_cfs = pd.DataFrame(imputer.fit_transform(X_cfs), columns=X_cfs.columns)\n",
    "\n",
    "# align target labels\n",
    "y_baseline = y_baseline.loc[X_baseline.index]\n",
    "y_cor = y_cor.loc[X_cor.index]\n",
    "y_cfs = y_cfs.loc[X_cfs.index]\n",
    "\n",
    "# encode target labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "datasets = {\n",
    "    \"Baseline Dataset\": (X_baseline, label_encoder.fit_transform(y_baseline)),\n",
    "    \"Correlation-Based Dataset\": (X_cor, label_encoder.fit_transform(y_cor)),\n",
    "    \"Wrapper Dataset\": (X_cfs, label_encoder.fit_transform(y_cfs)),\n",
    "}\n",
    "\n",
    "\n",
    "models = {\n",
    "    \"SVM\": SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000)\n",
    "}\n",
    "\n",
    "# train and evaluate each model using 5-fold CV\n",
    "results = {}\n",
    "\n",
    "for dataset_name, (X, y) in datasets.items():\n",
    "    print(f\"\\nTraining models on {dataset_name} with 5-fold cross-validation...\\n\")\n",
    "    \n",
    "    if len(X) == 0 or len(y) == 0:\n",
    "        print(f\"Skipping {dataset_name}: No valid samples after preprocessing.\\n\")\n",
    "        continue\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        try:\n",
    "            accuracies = []\n",
    "            \n",
    "            # perform 5-fold cross-validation\n",
    "            for train_index, test_index in skf.split(X, y):\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "                \n",
    "                model.fit(X_train, y_train)\n",
    "                \n",
    "                y_pred = model.predict(X_test)\n",
    "                \n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                accuracies.append(accuracy)\n",
    "            \n",
    "            mean_accuracy = sum(accuracies) / len(accuracies)\n",
    "            \n",
    "            if dataset_name not in results:\n",
    "                results[dataset_name] = {}\n",
    "            results[dataset_name][model_name] = mean_accuracy\n",
    "            \n",
    "            print(f\"Model: {model_name}\")\n",
    "            print(f\"Mean Accuracy (5-fold CV): {mean_accuracy:.2f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error training {model_name} on {dataset_name}: {e}\")\n",
    "\n",
    "# summary of results\n",
    "print(\"\\nSummary of Model Performance for Each Dataset (5-fold CV):\")\n",
    "for dataset_name, model_accuracies in results.items():\n",
    "    print(f\"\\n{dataset_name}:\")\n",
    "    for model_name, accuracy in model_accuracies.items():\n",
    "        print(f\"{model_name}: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models on Baseline Dataset...\n",
      "\n",
      "\n",
      "Tuning SVM...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "\n",
      "Baseline Dataset - SVM Tuned Accuracy: 0.47\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      1.00      0.56        10\n",
      "           1       1.00      0.40      0.57        10\n",
      "           2       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.47        30\n",
      "   macro avg       0.46      0.47      0.38        30\n",
      "weighted avg       0.46      0.47      0.38        30\n",
      "\n",
      "\n",
      "Tuning KNN...\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "\n",
      "Baseline Dataset - KNN Tuned Accuracy: 0.33\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        10\n",
      "           1       0.33      1.00      0.50        10\n",
      "           2       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.33        30\n",
      "   macro avg       0.11      0.33      0.17        30\n",
      "weighted avg       0.11      0.33      0.17        30\n",
      "\n",
      "\n",
      "Tuning Naive Bayes...\n",
      "\n",
      "Baseline Dataset - Naive Bayes Tuned Accuracy: 0.43\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.70      0.48        10\n",
      "           1       0.75      0.60      0.67        10\n",
      "           2       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.43        30\n",
      "   macro avg       0.37      0.43      0.38        30\n",
      "weighted avg       0.37      0.43      0.38        30\n",
      "\n",
      "\n",
      "Tuning Logistic Regression...\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/qiime2-amplicon-2024.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/qiime2-amplicon-2024.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/qiime2-amplicon-2024.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/qiime2-amplicon-2024.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/qiime2-amplicon-2024.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/qiime2-amplicon-2024.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline Dataset - Logistic Regression Tuned Accuracy: 0.47\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.50      0.42        10\n",
      "           1       0.67      0.60      0.63        10\n",
      "           2       0.43      0.30      0.35        10\n",
      "\n",
      "    accuracy                           0.47        30\n",
      "   macro avg       0.48      0.47      0.47        30\n",
      "weighted avg       0.48      0.47      0.47        30\n",
      "\n",
      "\n",
      "Training models on Correlation-Based Dataset...\n",
      "\n",
      "\n",
      "Tuning SVM...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "\n",
      "Correlation-Based Dataset - SVM Tuned Accuracy: 0.43\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.30      0.30        10\n",
      "           1       0.62      0.50      0.56        10\n",
      "           2       0.42      0.50      0.45        10\n",
      "\n",
      "    accuracy                           0.43        30\n",
      "   macro avg       0.45      0.43      0.44        30\n",
      "weighted avg       0.45      0.43      0.44        30\n",
      "\n",
      "\n",
      "Tuning KNN...\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "\n",
      "Correlation-Based Dataset - KNN Tuned Accuracy: 0.53\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.10      0.15        10\n",
      "           1       0.50      0.80      0.62        10\n",
      "           2       0.64      0.70      0.67        10\n",
      "\n",
      "    accuracy                           0.53        30\n",
      "   macro avg       0.49      0.53      0.48        30\n",
      "weighted avg       0.49      0.53      0.48        30\n",
      "\n",
      "\n",
      "Tuning Naive Bayes...\n",
      "\n",
      "Correlation-Based Dataset - Naive Bayes Tuned Accuracy: 0.53\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.40      0.42        10\n",
      "           1       0.57      0.80      0.67        10\n",
      "           2       0.57      0.40      0.47        10\n",
      "\n",
      "    accuracy                           0.53        30\n",
      "   macro avg       0.53      0.53      0.52        30\n",
      "weighted avg       0.53      0.53      0.52        30\n",
      "\n",
      "\n",
      "Tuning Logistic Regression...\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "\n",
      "Correlation-Based Dataset - Logistic Regression Tuned Accuracy: 0.53\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.60      0.55        10\n",
      "           1       0.62      0.50      0.56        10\n",
      "           2       0.50      0.50      0.50        10\n",
      "\n",
      "    accuracy                           0.53        30\n",
      "   macro avg       0.54      0.53      0.53        30\n",
      "weighted avg       0.54      0.53      0.53        30\n",
      "\n",
      "\n",
      "Training models on CFS Dataset...\n",
      "\n",
      "\n",
      "Tuning SVM...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "\n",
      "CFS Dataset - SVM Tuned Accuracy: 0.57\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.50      0.53        10\n",
      "           1       0.62      0.80      0.70        10\n",
      "           2       0.50      0.40      0.44        10\n",
      "\n",
      "    accuracy                           0.57        30\n",
      "   macro avg       0.56      0.57      0.56        30\n",
      "weighted avg       0.56      0.57      0.56        30\n",
      "\n",
      "\n",
      "Tuning KNN...\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "\n",
      "CFS Dataset - KNN Tuned Accuracy: 0.43\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.40      0.35        10\n",
      "           1       0.53      0.80      0.64        10\n",
      "           2       0.50      0.10      0.17        10\n",
      "\n",
      "    accuracy                           0.43        30\n",
      "   macro avg       0.45      0.43      0.38        30\n",
      "weighted avg       0.45      0.43      0.38        30\n",
      "\n",
      "\n",
      "Tuning Naive Bayes...\n",
      "\n",
      "CFS Dataset - Naive Bayes Tuned Accuracy: 0.57\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.50      0.48        10\n",
      "           1       0.67      0.80      0.73        10\n",
      "           2       0.57      0.40      0.47        10\n",
      "\n",
      "    accuracy                           0.57        30\n",
      "   macro avg       0.56      0.57      0.56        30\n",
      "weighted avg       0.56      0.57      0.56        30\n",
      "\n",
      "\n",
      "Tuning Logistic Regression...\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "\n",
      "CFS Dataset - Logistic Regression Tuned Accuracy: 0.37\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.10      0.13        10\n",
      "           1       0.50      0.60      0.55        10\n",
      "           2       0.31      0.40      0.35        10\n",
      "\n",
      "    accuracy                           0.37        30\n",
      "   macro avg       0.34      0.37      0.34        30\n",
      "weighted avg       0.34      0.37      0.34        30\n",
      "\n",
      "\n",
      "Summary of Improved Model Performance for Each Dataset:\n",
      "\n",
      "Baseline Dataset:\n",
      "SVM: 0.47\n",
      "KNN: 0.33\n",
      "Naive Bayes: 0.43\n",
      "Logistic Regression: 0.47\n",
      "\n",
      "Correlation-Based Dataset:\n",
      "SVM: 0.43\n",
      "KNN: 0.53\n",
      "Naive Bayes: 0.53\n",
      "Logistic Regression: 0.53\n",
      "\n",
      "CFS Dataset:\n",
      "SVM: 0.57\n",
      "KNN: 0.43\n",
      "Naive Bayes: 0.57\n",
      "Logistic Regression: 0.37\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Prepare the models with hyperparameter tuning\n",
    "tuned_models = {\n",
    "    \"SVM\": GridSearchCV(\n",
    "        SVC(probability=True, random_state=42),\n",
    "        param_grid={'C': [0.1, 1, 10], 'gamma': ['scale', 'auto'], 'kernel': ['linear', 'rbf']},\n",
    "        scoring='accuracy',\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "    ),\n",
    "    \"KNN\": GridSearchCV(\n",
    "        KNeighborsClassifier(),\n",
    "        param_grid={'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance']},\n",
    "        scoring='accuracy',\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "    ),\n",
    "    \"Naive Bayes\": GaussianNB(),  # No hyperparameters to tune\n",
    "    \"Logistic Regression\": GridSearchCV(\n",
    "        LogisticRegression(max_iter=1000),\n",
    "        param_grid={'C': [0.1, 1, 10]},\n",
    "        scoring='accuracy',\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Step 2: Train and evaluate models on datasets\n",
    "improved_results = {}\n",
    "\n",
    "for dataset_name, (X, y) in datasets.items():\n",
    "    print(f\"\\nTraining models on {dataset_name}...\\n\")\n",
    "    \n",
    "    if len(X) == 0 or len(y) == 0:\n",
    "        print(f\"Skipping {dataset_name}: No valid samples after preprocessing.\\n\")\n",
    "        continue\n",
    "    \n",
    "    # Address class imbalance\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_resampled = scaler.fit_transform(X_resampled)\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42, stratify=y_resampled)\n",
    "    \n",
    "    for model_name, model in tuned_models.items():\n",
    "        try:\n",
    "            print(f\"\\nTuning {model_name}...\")\n",
    "            \n",
    "            # Train the model (with hyperparameter tuning if applicable)\n",
    "            model.fit(X_train, y_train)\n",
    "            best_model = model.best_estimator_ if isinstance(model, GridSearchCV) else model\n",
    "            \n",
    "            # Evaluate the model\n",
    "            y_pred = best_model.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            \n",
    "            # Store the results\n",
    "            if dataset_name not in improved_results:\n",
    "                improved_results[dataset_name] = {}\n",
    "            improved_results[dataset_name][model_name] = accuracy\n",
    "            \n",
    "            print(f\"\\n{dataset_name} - {model_name} Tuned Accuracy: {accuracy:.2f}\")\n",
    "            print(\"Classification Report:\")\n",
    "            print(classification_report(y_test, y_pred))\n",
    "        except Exception as e:\n",
    "            print(f\"Error tuning {model_name} on {dataset_name}: {e}\")\n",
    "\n",
    "# Step 3: Display summary of improved results\n",
    "print(\"\\nSummary of Improved Model Performance for Each Dataset:\")\n",
    "for dataset_name, model_accuracies in improved_results.items():\n",
    "    print(f\"\\n{dataset_name}:\")\n",
    "    for model_name, accuracy in model_accuracies.items():\n",
    "        print(f\"{model_name}: {accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qiime2-amplicon-2024.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
